## MVP Evaluation Improvement Spec

### Comprehensive implementation instructions — ready for your AI coder

This is the full, actionable spec your coder needs to implement the MVP improvements. Follow it exactly. It covers DB changes, backend services, rule engine, LLM prompt + schema, confidence engine, transcript normalization, task queue, human-review capture, tests, observability, frontend contracts, and rollout checklist.

---

### 1. Priority summary (do these in order)

1. Lock deterministic evaluation (prompt/versioning, model settings, store raw payload).
2. Rule engine expansion and return format.
3. Human-review dataset capture and storage schema.
4. Strict LLM prompt + JSON schema; validate responses.
5. Confidence engine (5-signal).
6. Transcript normalization pipeline.
7. Replace in-memory thread queue with Cloud Tasks (or Pub/Sub).
8. Benchmarking tool & weekly report job.
9. UI changes for transparency and human review workflow.
10. Add tests and clear logging/audit.

---

### 2. DB schema changes / migrations

Add columns and tables necessary for reproducibility, review dataset, and audit.

**Migrations (SQLAlchemy/Alembic snippets):**

```sql
-- 1. evaluations: add reproducibility metadata & llm raw payload
ALTER TABLE evaluations
  ADD COLUMN prompt_id TEXT,
  ADD COLUMN prompt_version TEXT,
  ADD COLUMN model_version TEXT,
  ADD COLUMN model_temperature FLOAT DEFAULT 0.0,
  ADD COLUMN model_top_p FLOAT DEFAULT 1.0,
  ADD COLUMN llm_raw JSONB,
  ADD COLUMN rubric_version TEXT,
  ADD COLUMN evaluation_seed TEXT; -- optional trace id

-- 2. transcripts: add quality metrics
ALTER TABLE transcripts
  ADD COLUMN deepgram_confidence FLOAT,
  ADD COLUMN normalized_text TEXT;

-- 3. human_reviews
CREATE TABLE human_reviews (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  recording_id UUID REFERENCES recordings(id),
  evaluation_id UUID REFERENCES evaluations(id),
  reviewer_id UUID REFERENCES users(id),
  human_scores JSONB,         -- category -> score
  human_violations JSONB,     -- list of violations with evidence
  ai_scores JSONB,            -- snapshot of AI scores for comparison
  delta JSONB,                -- computed ai->human differences
  reviewer_notes TEXT,
  created_at TIMESTAMPTZ DEFAULT now()
);

-- 4. rule_engine_results
CREATE TABLE rule_engine_results (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  recording_id UUID REFERENCES recordings(id),
  evaluation_id UUID REFERENCES evaluations(id),
  rules JSONB,               -- map(rule_name -> {hit: bool, evidence, severity})
  created_at TIMESTAMPTZ DEFAULT now()
);
```

Add indexes on `recordings(status)`, `evaluations(requires_human_review)`, `human_reviews(created_at)`.

---

### 3. Prompt / model reproducibility contract

Enforce these for every LLM call:

- `temperature = 0.0`
- `top_p = 1.0` (omit sampling parameters that allow randomness)
- `model_version` stored (exact model ID & timestamp)
- `prompt_id` and `prompt_version` plugged into request and persisted
- Store raw LLM response in `evaluations.llm_raw` (full payload)
- Enforce strict JSON schema on model output and reject if invalid

DB fields used: `prompt_id`, `prompt_version`, `model_version`, `model_temperature`, `model_top_p`, `llm_raw`, `rubric_version`.

---

### 4. LLM output JSON schema (strict)

Use JSON Schema validation server-side. If model returns invalid JSON, mark confidence low and route to human.

```json
{
  "type": "object",
  "required": ["overall_score", "category_scores", "violations", "resolution", "explanations", "model_metadata"],
  "properties": {
    "overall_score": {"type": "number", "minimum": 0, "maximum": 100},
    "category_scores": {
      "type": "object",
      "patternProperties": {
        "^.+$": {"type": "number", "minimum": 0, "maximum": 100}
      }
    },
    "violations": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["rule_id", "description", "severity", "evidence"],
        "properties": {
          "rule_id": {"type": "string"},
          "description": {"type": "string"},
          "severity": {"type": "number"},
          "evidence": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "speaker": {"type": "string"},
                "text": {"type": "string"},
                "start": {"type": "number"},
                "end": {"type": "number"}
              }
            }
          }
        }
      }
    },
    "resolution": {"type": "string", "enum": ["resolved", "unresolved", "partial"]},
    "explanations": {"type": "object"},
    "model_metadata": {
      "type": "object",
      "required": ["model_id", "model_version", "prompt_id", "prompt_version"]
    }
  }
}
```

Implementation: use `jsonschema` to validate; if invalid: log and mark `requires_human_review = true` with reason `invalid_llm_schema`.

---

### 5. LLM prompt template (production-ready)

Use a 5-section prompt. Keep it strict; include schema in system message.

- **System (high-level rules):**
  - Always answer in valid JSON matching attached schema.
  - Use exact category names provided.
  - Don’t invent timestamps or speakers.
  - If uncertain about a criterion, put `null` and set `uncertainty_reason`.

- **Instruction-snippet (pseudocode):**

  ```
  SYSTEM: You are an evaluator. Output ONLY JSON. Follow the schema exactly.
  INPUT:
  - RUBRIC: (insert rubric object)
  - RULE_ENGINE: (insert deterministic flags with evidence)
  - TRANSCRIPT: (normalized transcript with diarized segments)
  - EXEMPLARS: (1-3 short examples pulled from human_reviews where model did well)
  TASK: Return JSON with overall_score [0-100], category_scores per rubric, violations array, resolution enum, explanations.
  ```

Implementation notes:

- Keep `TRANSCRIPT` under token budget; only include segments relevant to each rubric if call is long (smart chunking).
- Provide up to 3 exemplars (human-reviewed) chosen by similarity: same category violations or same keywords.

---

### 6. Rule engine spec (deterministic checks)

Implement rules as independent functions that return `{"hit": bool, "evidence": [...], "severity": int}`.

Rules to implement (priority order):

1. `greeting_within_15s`  
   Condition: agent speaks within first 15 seconds AND uses greeting keywords (hello, good morning, thank you for calling).  
   Evidence: earliest agent utterance text + timestamp.  
   Severity: 5 if missing.

2. `agent_identifies_self_or_company`  
   Condition: agent says their name or company within first 30s.  
   Severity: 4.

3. `apology_or_empathy_present`  
   Condition: presence of empathy tokens (sorry, i understand, that must be frustrating).  
   Severity: 3.

4. `hold_compliance`  
   Condition: when "hold" action occurs, agent says reason and approximate hold time.  
   Severity: 4.

5. `closing_and_confirmation`  
   Condition: agent confirms resolution and provides closing.  
   Severity: 5.

6. `dead_air`  
   Condition: continuous silence > 4s during agent/caller expected speech turn.  
   Severity: 2 per occurrence.

7. `interruptions`  
   Condition: speaker overlap detection above threshold; count interruptions by agent.  
   Severity: 2–4 depending on frequency.

8. `card_or_pii_mentioned`  
   Detect numeric sequences that match card or SSN patterns; flag for redaction.  
   Severity: 10 (critical).

9. `script_adherence`  
   If company script lines exist, match them; missing lines produce evidence.  
   Severity configurable.

Rule engine output format:

```json
{
  "greeting_within_15s": {
    "hit": false,
    "evidence": [{"speaker":"agent","text":"...","start":2.4}],
    "severity": 5
  },
  "...": {}
}
```

Implementation details:

- Use diarized speaker tags.
- Implement fuzzy matching (Levenshtein) for script matching 80% similarity threshold.
- Normalize punctuation and case before matching.

---

### 7. Confidence engine — 5-signal algorithm (code pseudocode)

**Signals:**

- `transcript_quality` (Deepgram confidence)
- `llm_reproducibility` (two runs compare)
- `rule_llm_agreement` (match between rule engine flags and LLM output)
- `category_consistency` (category scores follow rubric logic)
- `output_schema_valid` (boolean)

**Scoring:**

- Each signal returns 0..1 where 1 = high confidence.
- Weighted sum ⇒ `confidence_score = sum(weight_i * signal_i)`
- Thresholds:
  - `>= 0.8` = high (no review)
  - `0.5-0.8` = medium (consider sample review)
  - `< 0.5` = low (human review required)

**Weights (recommended):**

- transcript_quality: 0.25
- llm_reproducibility: 0.25
- rule_llm_agreement: 0.20
- category_consistency: 0.20
- output_schema_valid: 0.10

**Python pseudocode (implement in `confidence.py`):**

```python
def compute_confidence(transcript_confidence, llm_outputs, rule_results, category_scores):
    # 1) transcript_quality
    tq = min(1.0, transcript_confidence)  # e.g., 0.92

    # 2) llm_reproducibility: run LLM twice with same prompt and compare JSON diff ratio
    out1, out2 = llm_outputs
    reproducibility = json_similarity(out1, out2)  # returns 0..1

    # 3) rule_llm_agreement: check that LLM violations correspond to rule hits
    agreement = compute_rule_agreement(rule_results, out1['violations'])  # 0..1

    # 4) category_consistency: check logical constraints e.g. if greeting missing then greeting-related score should be 0
    consistency = compute_category_consistency(out1['category_scores'], rule_results)

    # 5) schema_valid
    schema_valid = int(validate_schema(out1))

    weights = [0.25, 0.25, 0.20, 0.20, 0.10]
    signals = [tq, reproducibility, agreement, consistency, schema_valid]
    confidence = sum(w*s for w,s in zip(weights, signals))
    return confidence
```

Notes:

- `json_similarity` can be computed as `1 - (diff_count / max_keys)` or use normalized token-level diff.
- Reproducibility check must be fast; consider running model twice only when transcript length < X tokens, otherwise use alternate heuristics (e.g., run a cheaper deterministic rubric-checker).

---

### 8. Transcript normalization pipeline

Implement pre-LLM step `normalize_transcript(transcript_raw)` that produces `transcript_normalized` persisted to DB.

Steps:

1. Remove filler tokens: regex `\b(uh|um|uhh|umm|you know)\b` replace with nothing (configurable).
2. Replace `[noise]`, `[inaudible]` with `{noise}` tokens but collapse runs.
3. Fix punctuation heuristics:
   - Insert full-stop after long pauses.
   - Capitalize sentence starts.
   - Merge consecutive segments by same speaker within 1.5s gap.
4. Trim transcript to relevant window for extremely long calls:
   - If duration > 20min, include:
     - First 120s
     - Last 120s
     - ±60s around every rule-engine flagged event (e.g., hold, card mention)
5. Tokenize and compute deepgram_confidence average and store.

Persist: `transcripts.normalized_text` and per-segment cleaned JSON.

---

### 9. Replace in-memory queue with Cloud Tasks (or Pub/Sub)

**Why:** Threads are unreliable and non-restartable; use Cloud Tasks (push) to Cloud Run endpoint.

**Flow:**

1. On upload, push task to Cloud Tasks with payload `{ "recording_id": ... }`.
2. Cloud Tasks invokes protected Cloud Run endpoint `/tasks/process_recording` authenticated by service account.
3. Worker picks up and calls `process_recording_task`. Use idempotency token in DB to avoid double-processing.

**Task handler pseudocode (FastAPI):**

```python
@router.post("/tasks/process_recording")
async def cloud_task_processor(payload: RecordingTask):
    recording_id = payload.recording_id
    # idempotency check
    if already_processing(recording_id):
        return {"status": "already_processing"}
    await process_recording_task(recording_id)
    return {"status": "ok"}
```

Retries: Configure Cloud Tasks retry policy and dead-letter queue to store failed payloads.

---

### 10. Human review API contract & UI fields

**API endpoints:**

- `GET /api/human_reviews/queue?limit=20` → returns pending human review items with AI snapshot and rule-engine results.
- `POST /api/human_reviews/{recording_id}` → payload:

  ```json
  {
    "reviewer_id": "uuid",
    "human_scores": { "greeting": 80, "empathy": 60, ... },
    "human_violations": [...],
    "reviewer_notes": "text",
    "corrections": {
      "category_scores": { ... },
      "overall_score": 85
    }
  }
  ```

On submit:

- Create `human_reviews` row.
- Mark `evaluation status = reviewed`.
- Save `ai_scores` snapshot and `delta` computed server-side.
- Trigger continuous_learning pipeline: add to fine_tuning queue.

**UI fields for HumanReview screen:**

- Top: AI summary block (overall_score, confidence, model metadata, rule_engine results)
- Center: Transcript scrubber with diarized speaker colors; timestamps clickable
- Right: Rubric editor with AI category scores editable by reviewer (inputs + comment)
- Bottom: Submit button with override checkbox
- Show AI vs Human diff after submit.

---

### 11. Benchmarking & validation tool

Create script `tools/benchmark.py`:

- Inputs: CSV of recording_ids with human-reviewed gold labels.
- For each id:
  - Re-run evaluation (or fetch existing AI evaluation).
  - Compare AI vs human per-category.
  - Compute: accuracy, precision/recall for violations, confusion matrix, MSE for score differences, % of calls flagged for human review.
- Output: JSON report and HTML summary with charts.

**Metrics to monitor weekly:**

- Overall score alignment (Pearson or Spearman) between AI and human.
- Category-level accuracy.
- Violation precision & recall.
- % of calls sent to human review (should be < ~30% after tuning).
- Mean confidence among reviewed vs non-reviewed.

---

### 12. Tests to add (backend)

- Unit tests for each rule function (positive and negative examples).
- Integration tests for `process_recording_task` using a small audio fixture and a Deepgram stub.
- Schema validation tests: send intentionally malformed LLM outputs and ensure the system flags review.
- Confidence engine tests: mock two LLM outputs and rule engine to verify threshold behavior.
- Human review roundtrip test: submit review, check `human_reviews` row and `evaluations.status`.

---

### 13. Observability & logging (must-have)

Every evaluation must log (structured JSON):

- recording_id, evaluation_id
- prompt_id + prompt_version
- model_version + runtime
- tokens_used and cost estimate
- transcript_confidence
- rule_engine summary
- LLM schema validation flag
- confidence_score and reason
- processing duration

Hook metrics to a metrics sink (Prometheus, Cloud Monitoring). Create alerts:

- 5% evaluations failing schema validation in 24h
- Median processing time > expected (e.g., 30s)
- Human review queue length > X

---

### 14. Cost control and throttling

- Add token_limit per evaluation and max_audio_length. If audio longer than X, trim per transcript normalization rules.
- Add `cost_tier` in `model_metadata` and allow fallback to cheaper model when complexity score low.
- Track daily token usage in DB and provide admin endpoint to pause LLM calls.

---

### 15. Frontend contract & UX changes

**Human Review page responsibilities:**

- Display AI summary (score, per-category, violations, confidence) — visible and auditable.
- Show rule engine detection block with evidence.
- Allow reviewer to edit category scores and violations; require at least one explanation when they change a score > 10 points.
- On submit: POST to `/api/human_reviews/{recording_id}`.

**Transparency UI:**

- On Results page, include:
  - “Why the AI scored this” panel listing rule hits and top 3 rationale sentences from LLM explanations.
  - Confidence bar + breakdown (transcript, reproducibility, rule agreement).
  - Add “Re-evaluate” action for supervisors.

---

### 16. Continuous learning pipeline (simplified for MVP)

- When a human_review is submitted, enqueue a `fine_tune_sample` with:
  - normalized transcript
  - ai_output
  - human_output
  - delta and reason
- Build few-shot exemplars automatically by selecting top N samples with highest delta per category.
- Do **not** retrain models in MVP — instead use exemplars in prompt (few-shot) and verify with benchmarking tool.

---

### 17. Example code snippets

**LLM wrapper (Python, simplified):**

```python
async def call_llm(prompt_text, model="gpt-x", temperature=0.0, top_p=1.0):
    resp = await llm_client.create_completion(
        model=model,
        prompt=prompt_text,
        temperature=temperature,
        top_p=top_p,
    )
    return resp  # store raw
```

**LLM double-run reproducibility helper:**

```python
def llm_reproducibility(prompt, model):
    out1 = call_llm(prompt, model)
    out2 = call_llm(prompt, model)
    # parse JSON; compute similarity ratio
    sim = json_similarity(out1_json, out2_json)
    return sim, out1_json
```

**Schema validation snippet:**

```python
from jsonschema import validate, ValidationError

def validate_llm_output(output_json, schema):
    try:
        validate(instance=output_json, schema=schema)
        return True, None
    except ValidationError as e:
        return False, str(e)
```

**Confidence compute (simplified):**

```python
def compute_confidence(...):
    # implement earlier pseudocode
    ...
```

---

### 18. Rollout plan (fast, safe)

1. Implement changes in a feature branch. Add migration and unit tests.
2. Deploy to staging. Run benchmark script on 200 historical calls. Tune thresholds until:
   - Overall alignment >= 0.7 correlation
   - Violation precision >= 0.75
   - Human review rate <= 30%
3. Demo to 1 pilot small BPO or internal QA team. Collect 200 human reviews in pilot.
4. Iterate on rules and exemplar selection; re-run benchmark.
5. Enable limited production with throttled throughput and strict logging.

---

### 19. Acceptance criteria (ship rules)

- Reproducibility: same call with same prompt+model produces identical parsed JSON (except when transcript changes).
- Schema acceptance rate >= 99% for valid inputs.
- Benchmarks: category-level F1 >= 0.7 on pilot dataset OR human review rate <= 30% with acceptable precision.
- Audit trail: every evaluation stores full model metadata and raw payload.
- Human review capture: all reviews stored in `human_reviews` table with delta computed.

---

### 20. Delivery checklist for your coder (exact tasks)

- [ ] DB migrations applied (new columns + tables).
- [ ] Rule engine functions implemented and unit-tested.
- [ ] Transcript normalization pipeline implemented.
- [ ] LLM wrapper enforces temperature=0 and stores raw payload.
- [ ] JSON Schema validator implemented and integrated.
- [ ] Confidence engine implemented and returns confidence_score and requires_human_review.
- [ ] Cloud Tasks (or Pub/Sub) integration and idempotent task handler.
- [ ] Human review endpoints and UI contract implemented.
- [ ] Benchmark script added and sample run documented.
- [ ] Structured logging and metrics instrumented.
- [ ] Tests added for all major pieces and CI passing.

---

### 21. Quick example: Full processing pipeline (sequence)

1. Upload audio → create `recordings` row (`status=queued`).
2. Push Cloud Task `{ recording_id }`.
3. Worker pulls task → mark recording processing.
4. Transcribe via Deepgram → persist raw + `deepgram_confidence`.
5. `normalize_transcript()` → persist `normalized_text`.
6. `rule_engine.evaluate()` → persist `rule_engine_results`.
7. Build LLM prompt with rubric, rule results, normalized transcript, exemplars (from `human_reviews`).
8. Call LLM twice (`temp=0`) → parse JSON → validate schema.
9. If schema invalid or reproducibility < threshold or transcript_confidence < threshold → set `requires_human_review`.
10. Compute `confidence_score` → persist.
11. Save evaluations with `llm_raw`, model metadata, category scores.
12. If `requires_human_review`, create `human_review` row (status pending) and notify reviewer.
13. Log all events in `audit_log`.
14. Frontend shows results. Reviewer edits → POST `/human_reviews/{id}` → store corrections and enqueue fine_tuning sample.

---

### 22. Example LLM few-shot exemplar selection rule

- Select top 3 `human_reviews` where `abs(ai_overall - human_overall) < 5` and `same_rule_hit` for the rule currently triggered.
- Use exemplar length <= 200 tokens.
- Add exemplar as `EXEMPLAR` block in prompt.

---

### 23. Final notes (no fluff)

- Focus on determinism, rule engine coverage, and data capture. Those three produce the most impact.
- Don’t train anything yet; use human corrections as few-shot exemplars and RAG. Retrain later after >5k labeled reviews.
- Keep prompt minimal and strict. Save every payload. Metrics will show whether you improved the model — not your gut.

---

